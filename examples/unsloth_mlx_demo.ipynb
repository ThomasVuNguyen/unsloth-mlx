{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth-Mac Complete Demo\n",
    "\n",
    "This notebook demonstrates the **complete Unsloth-Mac pipeline** - same API as Unsloth!\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Load ANY HuggingFace model\n",
    "- ‚úÖ LoRA configuration\n",
    "- ‚úÖ SFTTrainer (same as Unsloth)\n",
    "- ‚úÖ Real training\n",
    "- ‚úÖ Save methods (adapters, merged, GGUF)\n",
    "- ‚úÖ Works in Jupyter notebooks\n",
    "\n",
    "**Just like Unsloth, but for Apple Silicon!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install unsloth-mac\n",
    "```\n",
    "\n",
    "Or from source:\n",
    "```bash\n",
    "cd /path/to/unsloth-mac\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import - Same as Unsloth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth (CUDA): from unsloth import FastLanguageModel, SFTTrainer\n",
    "# Unsloth-Mac (MLX): Just change the import!\n",
    "\n",
    "from unsloth_mlx import FastLanguageModel, SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model - Same API as Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ANY HuggingFace model (not just mlx-community)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "print(f\"  Model: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA - Same API as Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters - exact same API as Unsloth\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # 0 is optimized\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "print(\"‚úì LoRA configured!\")\n",
    "print(f\"  Rank: {model.lora_config['r']}\")\n",
    "print(f\"  Alpha: {model.lora_config['lora_alpha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset - Same as Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In real use: dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "# For demo, we'll create a small dataset\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Python is a high-level programming language.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of AI.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"2+2 equals 4.\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"‚úì Dataset ready with {len(sample_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train with SFTTrainer - Same API as Unsloth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer - same API as Unsloth!\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=sample_data,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"./notebook_output\",\n",
    "    adapter_path=\"./notebook_adapters\",\n",
    "    iters=10,  # Small number for demo\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized!\")\n",
    "print(f\"  Learning rate: {trainer.learning_rate}\")\n",
    "print(f\"  Iterations: {trainer.iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model - this actually trains!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference - Same API as Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate response\n",
    "from mlx_lm import generate\n",
    "\n",
    "prompt = \"What is Python?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "response = generate(\n",
    "    model.model,\n",
    "    tokenizer,\n",
    "    prompt=formatted_prompt,\n",
    "    max_tokens=100,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Options - SAME AS UNSLOTH! üéâ\n",
    "\n",
    "Three save methods, just like Unsloth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Save LoRA Adapters Only (~100MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapters only - small file size\n",
    "# model.save_pretrained(\"lora_model\")\n",
    "\n",
    "print(\"Save adapters: model.save_pretrained('lora_model')\")\n",
    "print(\"‚úì Same API as Unsloth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Save Merged Model (Base + Adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model in HuggingFace format\n",
    "# Anyone can use with transformers.AutoModel\n",
    "\n",
    "# model.save_pretrained_merged(\n",
    "#     \"merged_16bit\",\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\"\n",
    "# )\n",
    "\n",
    "print(\"Save merged: model.save_pretrained_merged('merged_16bit', tokenizer)\")\n",
    "print(\"‚úì Same API as Unsloth!\")\n",
    "print(\"‚úì Others can use: transformers.AutoModel.from_pretrained('merged_16bit')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Export to GGUF for llama.cpp/Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF format\n",
    "# model.save_pretrained_gguf(\n",
    "#     \"model\",\n",
    "#     tokenizer,\n",
    "#     quantization_method=\"q4_k_m\"\n",
    "# )\n",
    "\n",
    "print(\"Save GGUF: model.save_pretrained_gguf('model', tokenizer, quantization_method='q4_k_m')\")\n",
    "print(\"‚úì Same API as Unsloth!\")\n",
    "print(\"‚úì Use with llama.cpp, Ollama, GPT4All, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: API Comparison\n",
    "\n",
    "### ‚úÖ What's the SAME:\n",
    "\n",
    "```python\n",
    "# Both Unsloth and Unsloth-Mac:\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "model = FastLanguageModel.get_peft_model(...)\n",
    "trainer = SFTTrainer(model=model, train_dataset=dataset, ...)\n",
    "trainer.train()\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.save_pretrained(\"lora_model\")\n",
    "model.save_pretrained_merged(\"merged\", tokenizer)\n",
    "model.save_pretrained_gguf(\"model\", tokenizer)\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è What's DIFFERENT:\n",
    "\n",
    "| Feature | Unsloth | Unsloth-Mac |\n",
    "|---------|---------|-------------|\n",
    "| Backend | CUDA/Triton | MLX |\n",
    "| Trainer | TRL-based | MLX-based |\n",
    "| Platform | NVIDIA GPUs | Apple Silicon |\n",
    "| Import | `from unsloth import` | `from unsloth_mlx import` |\n",
    "\n",
    "### üí° The Point:\n",
    "\n",
    "**Develop on Mac ‚Üí Deploy on CUDA**\n",
    "\n",
    "Just change the import line and your code works on both platforms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try with real datasets**: `load_dataset(\"yahma/alpaca-cleaned\")`\n",
    "2. **Train longer**: Increase `iters` or `num_train_epochs`\n",
    "3. **Try larger models**: Llama 3.2 3B, 7B, etc.\n",
    "4. **Share your model**: Upload to HuggingFace Hub\n",
    "5. **Deploy**: Export to GGUF and use with Ollama\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Unsloth-Mac GitHub](https://github.com/yourusername/unsloth-mac)\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai)\n",
    "- [MLX Documentation](https://ml-explore.github.io/mlx/)\n",
    "- [Examples](../examples/)\n",
    "\n",
    "---\n",
    "\n",
    "**Just like Unsloth, but for Mac! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
